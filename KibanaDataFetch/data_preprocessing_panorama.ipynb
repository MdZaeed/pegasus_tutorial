{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams.update({'font.size':16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dash_data_format(df, feature_list, target, regions, outfilename, region_column=\"app\"):\n",
    "    feature_list_and_target = feature_list[:]\n",
    "    feature_list_and_target.append(target)\n",
    "    \n",
    "    with open(outfilename, 'w') as txtfile:   \n",
    "        s = ','\n",
    "        s += ','.join([str(item) for item in regions])\n",
    "        s += '\\n'\n",
    "        txtfile.write(s)\n",
    "\n",
    "        for feat in feature_list_and_target:\n",
    "            if feat == region_column:\n",
    "                continue\n",
    "            row=feat# + \",\\\"\"\n",
    "            for reg in regions:\n",
    "                row += \",\\\"\"\n",
    "                ## GEt all rows that have the same kernel/region name\n",
    "                df_tmp = df[(df[region_column] == reg )] # DEBUG for empty values              \n",
    "                vals = []\n",
    "                \n",
    "                ## Now, make their values into a list\n",
    "                [vals.append(v) for v in df_tmp[feat].values]\n",
    "                s = ','.join([str(v) for v in vals])\n",
    "                row += s + \"\\\"\"\n",
    "            row += '\\n'\n",
    "            txtfile.write(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(var1, var2):\n",
    "    return (var1 / var2) ##This function can create an arbitrary combination of the two variables passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### AMD Q2: AAC data on MI-50\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "def read_input_manual4(dirname, model, optimizer, filelist):\n",
    "    input_prefix = [dirname+'/'] \n",
    "    \n",
    "    workloads = dict()\n",
    "    \n",
    "    for arch in range(len(input_prefix)):\n",
    "        print('ARCH: ', arch, input_prefix[arch])\n",
    "        input_file_list = []\n",
    "        for ind in range(len(filelist)):\n",
    "            filename = input_prefix[arch] + filelist[ind]\n",
    "            print('IND: ', ind, filename)\n",
    "            if path.exists(filename):\n",
    "                input_file_list.append(filename)\n",
    "                filename=\"\"\n",
    "            else:\n",
    "                print(filename, \"didn't exist\")\n",
    "        workloads[arch] = input_file_list\n",
    "    ##TODO: Change this\n",
    "    output_prefix = ['/Users/tanzima/Research/Stash/dashing-analysis-framework/data/dash_format_'+model+'_'+optimizer]#,\n",
    "#                     '/Users/tanzima/Research/Stash/dashing-analysis-framework/data/gilgamesh/dash_format_aac_basic_lstm',\n",
    "#                     '/Users/tanzima/Research/Stash/dashing-analysis-framework/data/gilgamesh/dash_format_aac_rnn']\n",
    "\n",
    "    return workloads, output_prefix # target_list and filter_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCH:  0 /home/tzed/Research/pegasus_tutorial/KibanaDataFetch/\n",
      "IND:  0 /home/tzed/Research/pegasus_tutorial/KibanaDataFetch/KickSmaple.xlsx\n",
      "{0: ['/home/tzed/Research/pegasus_tutorial/KibanaDataFetch/KickSmaple.xlsx']}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'kickstart'\n",
    "opt = '1000genome'\n",
    "filelist=[\n",
    "    'KickSmaple.xlsx'\n",
    "]\n",
    "dirname = '/home/tzed/Research/pegasus_tutorial/KibanaDataFetch'\n",
    "# workloads, output_prefix = read_input_manual4('/Users/tanzima/Research/AMD/DATA/mi-50/DATA-aac-july21/gd/prof/', 'basic_lstm', \"gd\", filelist)\n",
    "workloads, output_prefix = read_input_manual4(dirname, model_name, opt, filelist)\n",
    "\n",
    "\n",
    "########## Adam optimizer\n",
    "# filelist = ['s25-b2-l1024.csv', 's25-b2-l1536.csv', 's25-b32-l2048.csv', 's25-b32-l2560.csv',  's25-b32-l4096.csv', 's25-b64-l2048.csv']#, 'rnn/out-rnn-rnn-s30-b32-l128.csv']\n",
    "# workloads, output_prefix = read_input_manual4('/Users/tanzima/Research/AMD/DATA/mi-50/DATA-aac-july21/adam/prof/', 'basic_lstm', \"adam\", filelist)\n",
    "# workloads, output_prefix = read_input_pennant_mi100()\n",
    "print(workloads)\n",
    "#print(pd.read_csv(workloads[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_list_parser(region_list):\n",
    "    for i, kernelName in enumerate(region_list):\n",
    "        templateIdx = kernelName.find(\"<\")\n",
    "        if templateIdx == -1:\n",
    "            continue\n",
    "        namespaceIdx = kernelName.rfind(\"::\", 0, templateIdx)\n",
    "        assert(namespaceIdx != -1) # Shouldn't ever happen.\n",
    "        region_list[i] = kernelName[namespaceIdx+2:templateIdx]\n",
    "    print(region_list)\n",
    "    return region_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_features_and_regions(df_dict_param, filter_list, region_column=\"app\"):\n",
    "    # df_dist: list of workloads within a model\n",
    "    # filter_list: columns (metrics/counters) we don't want\n",
    "\n",
    "    features_dict = defaultdict(int)\n",
    "    common_region_dict = defaultdict(int)\n",
    "    regions_list_with_dup = [] # \"region\" = kernel\n",
    "    feature_list = [] # \"feature\" = column (metric/counter)\n",
    "    \n",
    "    #For each file\n",
    "    for key, df_instance in df_dict_param.items():\n",
    "        #read in the file\n",
    "        df_original2 = df_instance.copy()\n",
    "        ## REMOVE all columns that contain only 0s\n",
    "        df_original2 = df_original2.loc[:, (df_original2 != 0).any(axis=0)]\n",
    "        for v in df_original2.columns:\n",
    "            features_dict[v] += 1\n",
    "            \n",
    "        for v in df_original2[region_column]:\n",
    "            common_region_dict[v] += 1\n",
    "            #regions_list_with_dup.append(v)\n",
    "            \n",
    "    ## Finding the unique names of regions since regions_list_with_dup may have duplicate region names.\n",
    "    ##Here, region == kernel\n",
    "#     region_list = list(set(regions_list_with_dup))\n",
    "\n",
    "    ## FINDING COMMON FEATURES\n",
    "    common_feature_list = []\n",
    "    for k, v in features_dict.items():\n",
    "        if k in filter_list:\n",
    "            continue\n",
    "        if v == len(df_dict_param):\n",
    "            common_feature_list.append(k)\n",
    "\n",
    "            \n",
    "    ## FINDING COMMON Regions\n",
    "    common_region_list = []\n",
    "    for k, v in common_region_dict.items():\n",
    "        if v == len(df_dict_param):\n",
    "            common_region_list.append(k)\n",
    "\n",
    "        \n",
    "    return common_feature_list, common_region_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8503/3518108542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mdf_original2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#             print(df_original2['@timestamp'],df_original2['ts'] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n"
     ]
    }
   ],
   "source": [
    "## Reading and saving input\n",
    "df_dict_input = {}\n",
    "total_file_count = 0\n",
    "filter_list = ['index', 'dag_job_id', '@version', 'wf_label','rss', 'site', 'type', 'condor_job_id','exe', 'hostname', 'task_id', 'xformation', 'wf_uuid']\n",
    "for ind in range(len(workloads)):\n",
    "    file_name_list = workloads[ind]\n",
    "    for fname in file_name_list:\n",
    "        if not path.exists(fname):\n",
    "            print(fname, ' does not exist....')\n",
    "            continue\n",
    "        else:\n",
    "            df_original2 = pd.read_excel(fname)\n",
    "#             print(df_original2['@timestamp'],df_original2['ts'] )\n",
    "            \n",
    "#             newdf = newdf.sort_values(['pid', 'procs', 'threads', 'rank'], ascending = (True, True, True, True))\n",
    "#             newdf = newdf.sort_values(['@timestamp'], ascending = (True))\n",
    "            df_original2['@timestamp'] = pd.to_datetime(df_original2['@timestamp'], format=\"%Y-%m-%dT%H:%M:%S.%f\")#.sort_values()\n",
    "            newdf = df_original2.sort_values(['@timestamp'], ascending = (True))\n",
    "            print('----------', newdf.columns)\n",
    "            print('----------')\n",
    "             \n",
    "#             newdf['utimeTRatio'] = newdf['utimeT'] / newdf['stimeT']\n",
    "\n",
    "#             newdf['bwritePerT'] = newdf['bwrite'] / newdf['bwriteT']\n",
    "            newdf['vmTransPerSec'] = newdf['vmTrans'] / newdf['tsTrans']\n",
    "#             newdf['iowaitPerT'] = newdf['iowait'] / newdf['iowaitT']\n",
    "            newdf = newdf.drop(filter_list, axis=1)\n",
    "            newdf = newdf.loc[:, (newdf != 0).any(axis=0)]\n",
    "            \n",
    "            newdf.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            newdf = newdf.fillna(0)\n",
    "            \n",
    "            \n",
    "            newdf = newdf.drop(['procs', '@timestamp', 'event', 'rank', 'threads', 'tsTrans'], axis=1)\n",
    "            print(newdf.columns)\n",
    "\n",
    "            \n",
    "            ## REMOVE all columns that contain only 0s\n",
    "#             df_dict_input[total_file_count] = df_original2.groupby(['pid']).agg(func='mean')\n",
    "            df_dict_input[total_file_count] = newdf\n",
    "#             df_dict_input[total_file_count] = df_dict_input[total_file_count].reset_index()\n",
    "            total_file_count += 1\n",
    "print(total_file_count)\n",
    "print(df_dict_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8503/249367527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# import plotly.express as px\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtmpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dict_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# # tmpdf['bwritePerT'] = tmpdf['bwrite'] / tmpdf['bwriteT']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # tmpdf['breadPerT'] = tmpdf['bread'] / tmpdf['breadT']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# import plotly.express as px\n",
    "tmpdf = copy.deepcopy(df_dict_input[1])\n",
    "# # tmpdf['bwritePerT'] = tmpdf['bwrite'] / tmpdf['bwriteT']\n",
    "# # tmpdf['breadPerT'] = tmpdf['bread'] / tmpdf['breadT']\n",
    "\n",
    "\n",
    "# # tmpdf = tmpdf.drop(['pid', 'utime', 'syscw','event', 'stime', 'utimeT', 'stimeT', 'iowait', 'iowaitT', 'bwrite', 'bwriteT'], axis=1)\n",
    "\n",
    "\n",
    "# fig = px.scatter_matrix(tmpdf)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose workflow based on the characteristics of individual jobs. Each job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8503/2378764682.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(tmpdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtmpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m#returns a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "categories = ['bread','bwrite','vmTransPerSec',\n",
    "              'iowait', 'vm']\n",
    "# categories = ['bwritePerT','vmTransPerT',\n",
    "#               'iowaitPerT', 'vm']\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# print(tmpdf)\n",
    "x = tmpdf.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled, columns=tmpdf.columns)\n",
    "tmpdf = df\n",
    "#set opacity parameter based on index\n",
    "opacity_list = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "for ind in range(0,len(tmpdf['ts'])):\n",
    "\n",
    "    rlist = []\n",
    "\n",
    "    #Restructure data to have rlist = [i-th value from each column]\n",
    "    for cols in categories:\n",
    "        rlist.append(tmpdf[cols][ind])\n",
    "\n",
    "    #Add the i-th plot to the radar chart\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "      r=rlist,\n",
    "      theta=categories,\n",
    "      fill='toself',\n",
    "      opacity=opacity_list[ind%5],\n",
    "      name='A'+str(ind)\n",
    "    ))\n",
    "#     fig = px.line_polar(tmpdf, r=categories, theta=categories, line_close=True)\n",
    "    #Add interactive legends\n",
    "    fig.update_layout(\n",
    "      polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "          range=[0, 1]\n",
    "        )),\n",
    "      showlegend=True\n",
    "    )\n",
    "\n",
    "#Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code block for calculating the standard deivations across indexed variables to replace channel counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0 0      3.587133\n",
      "1      2.820570\n",
      "2     26.697662\n",
      "3      6.041099\n",
      "4      7.370629\n",
      "5      0.000000\n",
      "6      0.627186\n",
      "7      0.317365\n",
      "8      0.524976\n",
      "9      8.000000\n",
      "10     5.198301\n",
      "11     0.000000\n",
      "12     0.445559\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "1 0     0.039321\n",
      "1     0.085417\n",
      "2     2.647113\n",
      "3     1.412494\n",
      "4     3.718074\n",
      "5     0.150142\n",
      "6     0.000000\n",
      "7     0.049850\n",
      "8     0.118762\n",
      "9     0.064656\n",
      "10    1.000000\n",
      "11    2.027346\n",
      "12    0.000000\n",
      "13    0.052104\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "2 0      4.186853\n",
      "1      3.016892\n",
      "2     16.334386\n",
      "3     12.411254\n",
      "4      2.968864\n",
      "5      0.841799\n",
      "6      1.000000\n",
      "7      0.471181\n",
      "8      0.184631\n",
      "9      0.387865\n",
      "10     3.000000\n",
      "11     6.060330\n",
      "12     0.000000\n",
      "13     0.163903\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "3 0     0.008631\n",
      "1     0.015912\n",
      "2     1.544256\n",
      "3     1.000000\n",
      "4     3.417208\n",
      "5     0.516650\n",
      "6     0.000000\n",
      "7     0.057867\n",
      "8     0.113772\n",
      "9     0.047119\n",
      "10    0.000000\n",
      "11    2.085657\n",
      "12    0.000000\n",
      "13    0.013146\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "4 0     12.626374\n",
      "1      5.609715\n",
      "2     51.589850\n",
      "3      5.478201\n",
      "4     14.616051\n",
      "5      3.000000\n",
      "6      1.127291\n",
      "7      1.226547\n",
      "8      1.879917\n",
      "9     15.000000\n",
      "10    10.986627\n",
      "11     1.000000\n",
      "12     1.117074\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "5 0      6.131948\n",
      "1      2.224488\n",
      "2     23.759960\n",
      "3      6.564978\n",
      "4      7.643190\n",
      "5      2.000000\n",
      "6      0.315113\n",
      "7      0.460080\n",
      "8      1.253298\n",
      "9      9.000000\n",
      "10     7.725957\n",
      "11     0.000000\n",
      "12     0.381028\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "6 0     0.000000\n",
      "1     0.039980\n",
      "2     0.000000\n",
      "3     0.046697\n",
      "4     1.000000\n",
      "5     0.000000\n",
      "6     0.027007\n",
      "7     0.016966\n",
      "8     0.021151\n",
      "9     0.000000\n",
      "10    2.000260\n",
      "11    0.000000\n",
      "12    0.000440\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "7 0      1.613586\n",
      "1      2.745567\n",
      "2     18.450869\n",
      "3     13.868766\n",
      "4      2.745754\n",
      "5      1.502709\n",
      "6      0.000000\n",
      "7      0.515930\n",
      "8      0.266467\n",
      "9      0.216846\n",
      "10     5.000000\n",
      "11     3.610775\n",
      "12     0.000000\n",
      "13     0.158789\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "8 0     0.000080\n",
      "1     0.000720\n",
      "2     0.000000\n",
      "3     0.000000\n",
      "4     1.000000\n",
      "5     0.000000\n",
      "6     0.007913\n",
      "7     0.000000\n",
      "8     0.019141\n",
      "9     0.000000\n",
      "10    2.014813\n",
      "11    0.000000\n",
      "12    0.000000\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "9 0      6.718442\n",
      "1      4.927196\n",
      "2     50.488551\n",
      "3      5.528196\n",
      "4     14.297120\n",
      "5      0.000000\n",
      "6      0.794061\n",
      "7      0.536926\n",
      "8      1.500645\n",
      "9     15.000000\n",
      "10     9.552124\n",
      "11     0.000000\n",
      "12     1.092300\n",
      "Name: MemUnitBusy, dtype: float64\n",
      "134 3.5871328671328673\n",
      "134 2.8205697151424287\n",
      "134 26.697662337662337\n",
      "134 6.041099450274863\n",
      "134 7.370629370629371\n",
      "134 0.0\n",
      "134 0.6271864067966016\n",
      "134 0.31736526946107785\n",
      "134 0.5249756999327774\n",
      "134 8.0\n",
      "134 5.1983008495752125\n",
      "134 0.0\n",
      "134 0.44555879649978025\n",
      "134 0.03932067932067932\n",
      "134 0.08541729135432284\n",
      "134 2.647112887112887\n",
      "134 1.4124937531234383\n",
      "134 3.718073593073593\n",
      "134 0.15014152514152515\n",
      "134 0.0\n",
      "134 0.04985007496251874\n",
      "134 0.1187624750499002\n",
      "134 0.06465634708671718\n",
      "134 1.0\n",
      "134 2.027346326836582\n",
      "134 0.0\n",
      "134 0.05210372797378831\n",
      "134 4.186853146853147\n",
      "134 3.0168915542228887\n",
      "134 16.334385614385614\n",
      "134 12.411254372813593\n",
      "134 2.968864468864469\n",
      "134 0.8417991004497751\n",
      "134 1.0\n",
      "134 0.4711810761286024\n",
      "134 0.18463073852295409\n",
      "134 0.3878654094220672\n",
      "134 3.0\n",
      "134 6.060329835082459\n",
      "134 0.0\n",
      "134 0.16390298477644144\n",
      "134 0.008631368631368631\n",
      "134 0.015912043978010996\n",
      "134 1.5442557442557443\n",
      "134 1.0\n",
      "134 3.417207792207792\n",
      "134 0.5166500166500166\n",
      "134 0.0\n",
      "134 0.05786689988339164\n",
      "134 0.11377245508982035\n",
      "134 0.04711942006867608\n",
      "134 0.0\n",
      "134 2.085657171414293\n",
      "134 0.0\n",
      "134 0.013145802533264075\n",
      "134 12.626373626373626\n",
      "134 5.609715142428786\n",
      "134 51.58985014985015\n",
      "134 5.478200899550225\n",
      "134 14.616050616050616\n",
      "134 3.0\n",
      "134 1.1272905214059636\n",
      "134 1.2265469061876249\n",
      "134 1.87991678930252\n",
      "134 15.0\n",
      "134 10.986626686656672\n",
      "134 1.0\n",
      "134 1.117073560554601\n",
      "134 6.131948051948052\n",
      "134 2.224487756121939\n",
      "134 23.75996003996004\n",
      "134 6.564977511244378\n",
      "134 7.643190143190143\n",
      "134 2.0\n",
      "134 0.31511327669498584\n",
      "134 0.4600798403193613\n",
      "134 1.2532975418324521\n",
      "134 9.0\n",
      "134 7.725957021489255\n",
      "134 0.0\n",
      "134 0.38102848923162985\n",
      "134 0.0\n",
      "134 0.0399800099950025\n",
      "134 0.0\n",
      "134 0.04669665167416292\n",
      "134 1.0\n",
      "134 0.0\n",
      "134 0.027007329668499083\n",
      "134 0.016966067864271458\n",
      "134 0.021151278138115223\n",
      "134 0.0\n",
      "134 2.0002598700649674\n",
      "134 0.0\n",
      "134 0.0004395253126623247\n",
      "134 1.6135864135864135\n",
      "134 2.745567216391804\n",
      "134 18.45086913086913\n",
      "134 13.868765617191404\n",
      "134 2.7457542457542456\n",
      "134 1.5027086456771614\n",
      "134 0.0\n",
      "134 0.5159295352323838\n",
      "134 0.26646706586826346\n",
      "134 0.21684630548136843\n",
      "134 5.0\n",
      "134 3.6107746126936533\n",
      "134 0.0\n",
      "134 0.1587885084109162\n",
      "134 7.992007992007992e-05\n",
      "134 0.000719640179910045\n",
      "134 0.0\n",
      "134 0.0\n",
      "134 1.0\n",
      "134 0.0\n",
      "134 0.00791271031151091\n",
      "134 0.0\n",
      "134 0.019141412765029706\n",
      "134 0.0\n",
      "134 2.0148125937031485\n",
      "134 0.0\n",
      "134 0.0\n",
      "134 6.718441558441558\n",
      "134 4.9271964017991\n",
      "134 50.48855144855145\n",
      "134 5.528195902048975\n",
      "134 14.297119547119546\n",
      "134 0.0\n",
      "134 0.7940613026819924\n",
      "134 0.5369261477045908\n",
      "134 1.5006449737468432\n",
      "134 15.0\n",
      "134 9.552123938030984\n",
      "134 0.0\n",
      "134 1.0923003156590883\n"
     ]
    }
   ],
   "source": [
    "## Input processing and finding common features, and create a list of data frames for each model.\n",
    "#target_list = ['score']\n",
    "\n",
    "## Create derived target metrics\n",
    "#For each file\n",
    "##MOVE target_list and filter_list to be a list that is returned from read_input\n",
    "target_list = ['ts', 'GPUBusy', 'VALUBusy', 'MemUnitBusy', 'VALUUtilization', 'ratio1', 'ratio2']\n",
    "filter_list = ['BeginNs', 'EndNs', 'DispatchNs', 'BeginNs', 'EndNs', 'CompleteNs', 'Index','queue-id','queue-index', 'pid', 'tid', 'grd', 'wgr', 'lds', 'vgpr', 'sgpr', 'fbar', 'sig', 'obj']\n",
    "\n",
    "###### Decide which input dictionary we want to use, the one with stdDev variables or individual channels?\n",
    "\n",
    "# selected_df_dict = copy.deepcopy(df_dict_input)\n",
    "selected_df_dict = copy.deepcopy(df_with_stdDev_vars)\n",
    "\n",
    "\n",
    "common_top_region_list = []\n",
    "df_dict_updated = {}\n",
    "cumulative_workload_id = 0\n",
    "feature_list = {} ######### CHECK\n",
    "region_list = {} ################### CHECK\n",
    "shorten_region_list = {} ################### CHECK\n",
    "model_index = 0  # wot\n",
    "print(len(workloads))\n",
    "for model_index in range(len(workloads)): #For each model\n",
    "    \n",
    "    df_subdir = {}\n",
    "    for wl in range(len(workloads[model_index])): #For each input parameter in a model\n",
    "        df_subdir[wl] = selected_df_dict[cumulative_workload_id].copy()\n",
    "        cumulative_workload_id += 1\n",
    "    #Find the common regions and features. Should be consistent within a model.\n",
    "    ## TODO: Make a dictionary of feature list per model\n",
    "    feature_list[model_index], region_list[model_index] = find_common_features_and_regions(df_subdir, filter_list, \"KernelName\")\n",
    "#     shorten_region_list[model_index] = region_list_parser(region_list[model_index].copy())\n",
    "    shorten_region_list[model_index] = region_list[model_index]\n",
    "    \n",
    "    cumulative_df = pd.DataFrame(columns=feature_list[model_index])\n",
    "    \n",
    "    for wl in range(len(workloads[model_index])): #For each input parameter in a model\n",
    "        #read in the file\n",
    "        df_original2 = df_subdir[wl].copy() #The dataset is already there. \n",
    "        ## REMOVE all columns that contain only 0s\n",
    "        df_original2 = df_original2.loc[:, (df_original2 != 0).any(axis=0)]\n",
    "\n",
    "        #Add new targets-- for AMD work, the target 'ts' is computed based on 'BeginNs', 'EndNs'\n",
    "        df_original2['ts'] = df_original2['EndNs'] - df_original2['BeginNs']\n",
    "        #df_original2['ratio1'] = 100*df_original2['VALUBusy'] / df_original2['GPUBusy'] #100*df_original2['GRBM_GUI_ACTIVE']/df_original2['GRBM_COUNT']\n",
    "        #df_original2['ratio2'] = 100*df_original2['VALUBusy'] / df_original2['MemUnitBusy'] #100*df_original2['GRBM_GUI_ACTIVE']/df_original2['GRBM_COUNT']\n",
    "#         df_original2['shortenKernelName'] = #region_list_parser(df_original2['KernelName'].copy())#region_list[model_index].copy())\n",
    "        \n",
    "        ## Just keep the top 5 regions according to their 'ts'\n",
    "#         df_original2.sort_values(by=['ts'], inplace=True, ascending=False)\n",
    "        \n",
    "        ## Only keep those rows that belong to the common region names\n",
    "#         for reg in shorten_region_list[model_index]:\n",
    "#             df_original2 = df_original2[(df_original2['shortenKernelName'] == reg )] \n",
    "        \n",
    "        ## Now, among those common regions, choose top 15 kernels.\n",
    "#         top_regions = (df_original2.head(15))['KernelName']\n",
    "#         region_list = top_regions\n",
    "        ## Now, create a list with the top kernel names \n",
    "        common_top_region_list.append([reg for reg in region_list[model_index]])\n",
    "\n",
    "        \n",
    "        # Now, appending all rows from different workloads to a stack workloads. Concat seems to be the way.\n",
    "        cumulative_df = cumulative_df.append(df_original2, ignore_index = True)\n",
    "        print(wl, df_original2['MemUnitBusy'])        \n",
    "\n",
    "    df_dict_updated[model_index] = cumulative_df\n",
    "        \n",
    "        \n",
    "for numx in df_dict_updated[0]['MemUnitBusy']: \n",
    "    print(len(df_dict_updated[0]['MemUnitBusy']), numx)\n",
    "\n",
    "# print(common_top_region_list[0])\n",
    "common_region_list = list(set(common_top_region_list[0]))\n",
    "# print(common_region_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Generating /Users/tanzima/Research/Stash/dashing-analysis-framework/data/aac/dash_format_aac_basic_lstm_adam_ts.csv...\n",
      "Generating /Users/tanzima/Research/Stash/dashing-analysis-framework/data/aac/dash_format_aac_basic_lstm_adam_GPUBusy.csv...\n",
      "Generating /Users/tanzima/Research/Stash/dashing-analysis-framework/data/aac/dash_format_aac_basic_lstm_adam_MemUnitBusy.csv...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## Input processing and finding common features\n",
    "\n",
    "#target_list = ['score']\n",
    "\n",
    "## Create derived target metrics\n",
    "#For each file\n",
    "# target_list = ['ts', 'GPUBusy', 'VALUBusy', 'MemUnitBusy', 'VALUUtilization', 'ratio1', 'ratio2']\n",
    "target_list = ['ts', 'GPUBusy', 'MemUnitBusy']\n",
    "\n",
    "# config_list = ['basic_lstm', 'lstm', 'rnn']\n",
    "config_list = ['pennent']\n",
    "# target_list = ['ts']\n",
    "cumulative_index = 0\n",
    "print(len(df_dict_updated))\n",
    "for model_index in range(len(df_dict_updated)):#range(len(workloads)): #For each model\n",
    "    df_original2 = df_dict_updated[model_index]\n",
    "    for target in target_list:\n",
    "        df_original3 = df_original2[feature_list[model_index]]\n",
    "        ##Make sure to fill up all Nan values with 0\n",
    "        df_original3 = df_original3.fillna(0)\n",
    "\n",
    "        #Add a target at a time to original3\n",
    "        df_original3[target] = df_original2[target]\n",
    "\n",
    "        outfilename = output_prefix[model_index] + '_' + target+'.csv'\n",
    "        print('Generating ' + outfilename + '...')\n",
    "        #generate_dash_data_format(df_original3, feature_list, target, region_list, outfilename, \"KernelName\")\n",
    "        ## TODO: PASS the following function feature_list[ind] and region_list[ind]\n",
    "        generate_dash_data_format(df_original3, feature_list[model_index], target, region_list[model_index], outfilename, \"KernelName\")\n",
    "        \n",
    "        #generate_config_block_for_this_input(config_list[model_index] + '_'+target, outfilename, target)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## AMD Q2 block-- generate 1 dash file per input config or Workload. This is so that we can correlate the resource\n",
    "#### importance values with FOM\n",
    "\n",
    "##MOVE target_list and filter_list to be a list that is returned from read_input\n",
    "target_list = ['ts', 'GPUBusy', 'VALUBusy', 'MemUnitBusy', 'VALUUtilization', 'ratio1', 'ratio2']\n",
    "filter_list = ['BeginNs', 'EndNs', 'DispatchNs', 'BeginNs', 'EndNs', 'CompleteNs', 'Index','queue-id','queue-index', 'pid', 'tid', 'grd', 'wgr', 'lds', 'vgpr', 'sgpr', 'fbar', 'sig', 'obj']\n",
    "\n",
    "###### Decide which input dictionary we want to use, the one with stdDev variables or individual channels?\n",
    "\n",
    "# selected_df_dict = copy.deepcopy(df_dict_input)\n",
    "selected_df_dict = copy.deepcopy(df_with_stdDev_vars)\n",
    "df_subdir = {}\n",
    "df_dict_updated = {}\n",
    "cumulative_workload_id = 0\n",
    "feature_list = {} ######### CHECK\n",
    "region_list = {} ################### CHECK\n",
    "shorten_region_list = {} ################### CHECK\n",
    "\n",
    "\n",
    "for model_index in range(len(workloads)): #For each model lstm, basic_lstm, rnn, ....\n",
    "    for wl in range(len(workloads[model_index])): #For each input parameter in a model\n",
    "        df_subdir[wl] = selected_df_dict[cumulative_workload_id].copy()\n",
    "        feature_list[wl], region_list[wl] = find_common_features_and_regions(df_subdir, filter_list, \"KernelName\")\n",
    "        shorten_region_list[wl] = region_list[wl]\n",
    "\n",
    "        df_original2 = df_subdir[wl].copy() #The dataset is already there. \n",
    "        ## REMOVE all columns that contain only 0s\n",
    "        df_original2 = df_original2.loc[:, (df_original2 != 0).any(axis=0)]\n",
    "\n",
    "        #Add new targets-- for AMD work, the target 'ts' is computed based on 'BeginNs', 'EndNs'\n",
    "        df_original2['ts'] = df_original2['EndNs'] - df_original2['BeginNs']\n",
    "        df_original2['ratio1'] = 100*df_original2['VALUBusy'] / df_original2['GPUBusy'] #100*df_original2['GRBM_GUI_ACTIVE']/df_original2['GRBM_COUNT']\n",
    "        df_original2['ratio2'] = 100*df_original2['VALUBusy'] / df_original2['MemUnitBusy'] #100*df_original2['GRBM_GUI_ACTIVE']/df_original2['GRBM_COUNT']\n",
    "\n",
    "        #generate_dash_data_format(df_original2, feature_list[wl], \"ts\", region_list[wl], \"sample_dash.csv\", \"KernelName\")\n",
    "        df_dict_updated[cumulative_workload_id] = df_original2#.deepcopy()\n",
    "\n",
    "        cumulative_workload_id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      3.587133\n",
      "1      2.820570\n",
      "2     26.697662\n",
      "3      6.041099\n",
      "4      7.370629\n",
      "5      0.000000\n",
      "6      0.627186\n",
      "7      0.317365\n",
      "8      0.524976\n",
      "9      8.000000\n",
      "10     5.198301\n",
      "11     0.000000\n",
      "12     0.445559\n",
      "Name: MemUnitBusy, dtype: float64 0      3.587133\n",
      "1      2.820570\n",
      "2     26.697662\n",
      "3      6.041099\n",
      "4      7.370629\n",
      "5      0.000000\n",
      "6      0.627186\n",
      "7      0.317365\n",
      "8      0.524976\n",
      "9      8.000000\n",
      "10     5.198301\n",
      "11     0.000000\n",
      "12     0.445559\n",
      "Name: MemUnitBusy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (df_dict_updated[0]['MemUnitBusy'], df_dict_updated[0]['MemUnitBusy'])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      3.587133\n",
      "1      2.820570\n",
      "2     26.697662\n",
      "3      6.041099\n",
      "4      7.370629\n",
      "5      0.000000\n",
      "6      0.627186\n",
      "7      0.317365\n",
      "8      0.524976\n",
      "9      8.000000\n",
      "10     5.198301\n",
      "11     0.000000\n",
      "12     0.445559\n",
      "Name: MemUnitBusy, dtype: float64 0      3.587133\n",
      "1      2.820570\n",
      "2     26.697662\n",
      "3      6.041099\n",
      "4      7.370629\n",
      "5      0.000000\n",
      "6      0.627186\n",
      "7      0.317365\n",
      "8      0.524976\n",
      "9      8.000000\n",
      "10     5.198301\n",
      "11     0.000000\n",
      "12     0.445559\n",
      "Name: MemUnitBusy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (df_dict_input[0]['MemUnitBusy'], df_dict_input[0]['MemUnitBusy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'Cijk_Ailk_Bjlk_SB_MT128x128x8_SE_1LDSB0_APM1_AF0EM1_AF1EM1_AMAS2_ASBE01_ASCE01_ASEM1_BL0_DTL0_DVO0_EPS0_FL0_GRVW4_GSU1_ISA000_IU1_K1_KLS_LBSPP0_LPA0_LPB0_LDL1_LRVW4_MDA2_NLCA1_NLCB1_ONLL1_PBD0_PK0_PGR1_PLR1_RK0_SIA1_SU32_SUM0_SUS256_SRVW0_SVW4_SNLL0_TT8_8_TLDS0_USFGRO0_VAW1_VS1_VW4_WSGRA0_WSGRB0_WG16_16_1_WGM1.kd', 'Cijk_Ailk_Bljk_SB_MT64x64x16_SE_1LDSB0_APM1_AF0EM1_AF1EM1_AMAS3_ASBE01_ASCE01_ASEM1_BL1_DTL0_DVO0_EPS1_FL1_GRVW4_GSU1_ISA908_IU1_K1_KLA_LBSPP0_LPA0_LPB4_LDL1_LRVW4_MDA2_NLCA1_NLCB1_ONLL1_PBD0_PK0_PGR1_PLR1_RK0_SIA1_SU32_SUM0_SUS256_SRVW0_SVW4_SNLL1_TT4_8_TLDS0_USFGRO0_VAW1_VS1_VW4_WSGRA0_WSGRB0_WG16_8_2_WGM8.kd', 'Cijk_Alik_Bljk_SB_MT64x64x16_SE_1LDSB0_APM1_AF0EM1_AF1EM1_AMAS3_ASBE01_ASCE01_ASEM1_BL1_DTL0_DVO0_EPS1_FL1_GRVW4_GSU1_ISA908_IU1_K1_KLA_LBSPP0_LPA4_LPB4_LDL1_LRVW4_MDA2_NLCA1_NLCB1_ONLL1_PBD0_PK0_PGR1_PLR1_RK0_SIA1_SU32_SUM0_SUS256_SRVW0_SVW4_SNLL1_TT4_4_TLDS0_USFGRO1_VAW1_VS1_VW4_WSGRA0_WSGRB0_WG16_16_1_WGM8.kd', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 1: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 2: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 3: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 4: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 5: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 6: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 7: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 8: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd'], 9: ['BiasGradNHWC_SharedAtomics', 'BiasNHWCKernel', 'ColumnReduceKernel', 'CwiseFusedMulAdd2Kernel', 'CwiseFusedMulAddKernel', 'EigenMetaKernel', 'FillPhiloxRandomKernelLaunch', 'SplitOpKernel', '__amd_rocclr_copyBuffer.kd', '__amd_rocclr_fillBuffer.kd']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ind in range(len(workloads)):\n",
    "    print(region_list)\n",
    "len(region_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for key, val in df_dict_updated.items():\n",
    "    print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  player  points  extra\n",
      "0      A      12      1\n",
      "1      B       5      5\n",
      "2      A      13      3\n",
      "3      B      17      7\n",
      "4      B      27      2\n",
      "  player     points     extra\n",
      "0      A  12.500000  2.000000\n",
      "1      B  16.333333  4.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create two DataFrames\n",
    "\n",
    "df1 = pd.DataFrame({'player': ['A', 'B', 'A', 'B', 'B'],\n",
    "                    'points':[12, 5, 13, 17, 27],\n",
    "                   'extra':[1, 5, 3, 7, 2]})\n",
    "\n",
    "df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],\n",
    "                    'points':[24, 26, 27, 27, 12]})\n",
    "\n",
    "#\"stack\" the two DataFrames together\n",
    "df3 = pd.concat([df1,df2], axis=0, ignore_index=True)\n",
    "\n",
    "print (df1)\n",
    "# A = df1.append(df2, ignore_index=True)\n",
    "# print(A.append(df3, ignore_index=True))\n",
    "\n",
    "# df = df1.groupby(['player']).agg({'extra':'mean','points':'mean'})\n",
    "df = df1.groupby(['player']).agg(func='mean')\n",
    "df = df.reset_index()\n",
    "print (df)\n",
    "\n",
    "# print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_dash_data_format(df_dict_updated[0], feature_list[0], \"ts\", region_list[0], \"sample_dash.csv\", \"KernelName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dict_updated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config_block_for_this_input(config_name, datafilename, target):\n",
    "    print(config_name + ':')\n",
    "    print('    data: ', datafilename)\n",
    "    print('    tasks: ')\n",
    "    print('      - modules.resource_score.compute_rsm_task_all_regions')\n",
    "    print('      - viz.barchart.create_rsm_error_barchart')\n",
    "    print('      - viz.barchart.create_rsm_percent_barchart')\n",
    "    print('      - viz.sunburst2.sunburst')\n",
    "    print('      - viz.linechart3.plot_raw_target_values')\n",
    "    print('      - viz.linechart3.raw_values_per_config')\n",
    "    print('    name: ', target)\n",
    "    print('    target: ', target)\n",
    "    \n",
    "    if target == 'ts':\n",
    "        print('    compute_target: modules.compute_target.compute_runtime')\n",
    "    else:\n",
    "        print('    compute_target: modules.compute_target.compute_inverse_target')\n",
    "    print('##############################')\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## AMD Q2: OUTPUT PROESSING: 1 file per input per model.\n",
    "\n",
    "#target_list = ['score']\n",
    "\n",
    "## Create derived target metrics\n",
    "#For each file\n",
    "filelist = ['s30-b32-l128', 's30-b128-l128', 's30-b32-l512', 's30-b128-l512', 's30-b32-l1024','s30-b128-l1024']\n",
    "#target_list = ['ts', 'GPUBusy', 'VALUBusy', 'MemUnitBusy', 'VALUUtilization', 'ratio1', 'ratio2']\n",
    "target_list = ['ts']\n",
    "cumulative_index = 0\n",
    "for model_index in range(len(df_dict_updated)):#range(len(workloads)): #For each model\n",
    "    df_original2 = df_dict_updated[model_index]\n",
    "    for target in target_list:\n",
    "        df_original3 = df_original2[feature_list[model_index]]\n",
    "        ##Make sure to fill up all Nan values with 0\n",
    "        df_original3 = df_original3.fillna(0)\n",
    "\n",
    "        #Add a target at a time to original3\n",
    "        df_original3[target] = df_original2[target]\n",
    "\n",
    "        #\n",
    "        outfilename = output_prefix[ind] + '_' + target +'_'+filelist[model_index]+'.csv'\n",
    "        #print('Generating ' + outfilename + '...')\n",
    "        generate_dash_data_format(df_original3, feature_list, target, region_list, outfilename, \"KernelName\")\n",
    "        ## TODO: PASS the following function feature_list[ind] and region_list[ind]\n",
    "        #generate_dash_data_format(df_original3, feature_list[model_index], target, region_list[model_index], outfilename, \"KernelName\")\n",
    "        print('    - ', filelist[model_index])\n",
    "        #generate_config_block_for_this_input(filelist[model_index], outfilename, target)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## AMD Q2 -- feature compression and feature correaltion analysis --- \n",
    "## input: df_dict_updated --> each model has all input combined into it.\n",
    "colors=['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for model_index in range(len(df_dict_updated)):#range(len(workloads)): #For each model\n",
    "    df_original2 = df_dict_updated[model_index]\n",
    "    plot_df(df_original2, 'ts', colors[model_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(input_df, target, c):\n",
    "    fig, ax = plt.subplots()\n",
    "    input_df[target].plot(ax=ax, kind='bar', color = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns info sieve output with nh components    \n",
    "def infoSieveX(X,nh):\n",
    "        identifier= 'Using Information Sieve %d Components'%nh\n",
    "        s = linearsieve.Sieve(n_hidden=nh, verbose=0).fit(X + 0.05*np.random.randn(X.shape[0],X.shape[1]))\n",
    "        X_lat = s.transform(X) \n",
    "        return X_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_dict_updated[0]\n",
    "col_std_list = [ col for col in df2.columns if '_std' in col]\n",
    "for c in col_std_list:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('in.txt', sep=\" \")\n",
    "file1 = open(\"event_names_amd.txt\",\"w\")\n",
    "\n",
    "for k in df:\n",
    "    #print (k)\n",
    "    file1.write(k+\"\\n\")\n",
    "file1.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Special processing block for Q2: AMD report\n",
    "##Input: res_imp_rnn.csv file which contains resource importance for all the kernels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fom_lstm_output(filename):\n",
    "\n",
    "    ###### Read and parse FOM for LSTM / RNN benchmark's output\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with open(filename, \"r\") as fom_file:\n",
    "        read_next_line = 0\n",
    "        for line in fom_file:\n",
    "    #         print(line)\n",
    "            if read_next_line == 1: #If I am supposed to read this line \n",
    "                total += np.float((line.split())[4])\n",
    "                read_next_line = 0\n",
    "                count += 1\n",
    "            elif \"Forward + Backward\" in line:\n",
    "                read_next_line = 1\n",
    "    return 1.0/(total/count)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'prof-mi100-basic_lstm-adam-b4-l512-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b4-l64-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b8-l256-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b8-l64-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b8-l1024-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b8-l512-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b8-l32-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b4-l256-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b4-l32-s25.csv',\n",
      "'prof-mi100-basic_lstm-adam-b4-l1024-s25.csv',\n"
     ]
    }
   ],
   "source": [
    "# read_fom_lstm_output('/Users/tanzima/Research/AMD/DATA/mi-50/DATA-aac-july21/results')\n",
    "import os\n",
    "model_names = ['basic_lstm', 'rnn', 'lstm']\n",
    "for filename in os.listdir('/Users/tanzima/Research/AMD/DATA/mi-100/prof/adam/basic_lstm'):\n",
    "    print (\"'\"+filename+\"',\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
